# Dataset settings
dataset_id: "stanfordnlp/sst2"

# Model settings
model_name: "meta-llama/Llama-3.2-1B"
max_length: 64
dtype: "torch.bfloat16"

# Training hyperparameters
learning_rate: 2e-5
batch_size: 16
gradient_checkpointing: False
num_epochs: 1
ignored_index: -100
optim: "adamw_torch"

# LoRA setup
lora_r: 4
lora_alpha: 8
lora_target_modules: ["q_proj", "v_proj"]
lora_task_type: "CAUSAL_LM"

# Other settings
seed: 23
output_dir: "blacksmith/experiments/torch/llama/ffe"
wandb_project: "llama-finetuning-ffe"
wandb_run_name: "tt-llama-test"
wandb_watch_mode: "all"
wandb_log_freq: 1000
model_to_wandb: False
save_strategy: "epoch"
logging_steps: 100
do_train: True
use_tt: True
